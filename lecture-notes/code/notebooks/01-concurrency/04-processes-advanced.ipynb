{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7700207f",
   "metadata": {},
   "source": [
    "\n",
    "# Multiprocessing with Shared Memory\n",
    "\n",
    "This notebook shows how to avoid copying a **large dataset** to every worker process by\n",
    "using `multiprocessing.shared_memory`.\n",
    "\n",
    "We load data **once**, let each process **attach**\n",
    "to the shared buffer by **name**, compute on disjoint slices, and aggregate results.\n",
    "\n",
    "##### Plan\n",
    "1. Create big array and place it into shared memory\n",
    "1. Build coarse **chunks**\n",
    "1. Dispatch with `ProcessPoolExecutor`, collect partial results, verify, clean up\n",
    "\n",
    "##### Single thread in every process\n",
    "\n",
    "First, we set single-threaded environment vars (to avoid nested BLAS threads).\n",
    "Numpy may run C code, that allows multiple threads to run in parallel.\n",
    "Running many processes in parallel and each process (that may run numpy) running\n",
    "many threads in parallel, will lead to oversubscription and saturation of the \n",
    "available computational resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e92fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import importlib\n",
    "import multiprocessing as mp\n",
    "\n",
    "from multiprocessing import shared_memory\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from mp_tasks import chunk_sum\n",
    "\n",
    "# (Optional) Reduce nested threading in BLAS/OpenMP libraries before importing NumPy\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b0d020",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1 — Create a big array and place it in shared memory\n",
    "\n",
    "We create a large NumPy array.\n",
    "\n",
    "We allocate a shared-memory buffer and copy the data **once**.\n",
    "\n",
    "We retain the **name**, **shape**, and **dtype** so workers can reattach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6011795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Shared memory created\n",
      "  name   : psm_cdeed9db\n",
      "  shape  : (20000000,)\n",
      "  dtype  : float32\n",
      "  bytes  : 80,000,000\n"
     ]
    }
   ],
   "source": [
    "# Simulate a heavy dataset (adjust size as needed)\n",
    "rng = np.random.default_rng(0)\n",
    "A = rng.standard_normal(20_000_000, dtype=np.float32)  # ~76 MB\n",
    "\n",
    "# Allocate shared memory once and copy data into it\n",
    "shm = shared_memory.SharedMemory(create=True, size=A.nbytes)\n",
    "shm_data = np.ndarray(A.shape, dtype=A.dtype, buffer=shm.buf)\n",
    "shm_data[:] = A  # one-time copy\n",
    "\n",
    "print(\"✅ Shared memory created\")\n",
    "print(f\"  name   : {shm.name}\")\n",
    "print(f\"  shape  : {shm_data.shape}\")\n",
    "print(f\"  dtype  : {str(shm_data.dtype)}\")\n",
    "print(f\"  bytes  : {A.nbytes:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20911b26",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 — Build coarse-grained chunks\n",
    "\n",
    "We split the array into a modest number of contiguous slices (≈ 1–2 per worker).\n",
    "This keeps each task **coarse**, avoiding excessive pickling/IPC overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b24b3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Built 24 tasks\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 0, 833333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 833333, 1666666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 1666666, 2500000)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 2500000, 3333333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 3333333, 4166666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 4166666, 5000000)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 5000000, 5833333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 5833333, 6666666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 6666666, 7500000)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 7500000, 8333333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 8333333, 9166666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 9166666, 10000000)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 10000000, 10833333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 10833333, 11666666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 11666666, 12500000)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 12500000, 13333333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 13333333, 14166666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 14166666, 15000000)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 15000000, 15833333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 15833333, 16666666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 16666666, 17500000)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 17500000, 18333333)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 18333333, 19166666)\n",
      "task: ('psm_cdeed9db', (20000000,), 'float32', 19166666, 20000000)\n"
     ]
    }
   ],
   "source": [
    "n_workers = os.cpu_count() or 4\n",
    "n_chunks = max(n_workers * 2, 1)  # aim for 1–2 chunks per worker\n",
    "\n",
    "n_items = shm_data.size\n",
    "edges = np.linspace(0, n_items, n_chunks + 1, dtype=int)\n",
    "\n",
    "tasks = [\n",
    "    (shm.name, shm_data.shape, str(shm_data.dtype), int(edges[i]), int(edges[i+1]))\n",
    "    for i in range(n_chunks)\n",
    "]\n",
    "\n",
    "print(f\"✅ Built {len(tasks)} tasks\")\n",
    "for task in tasks:  \n",
    "    print(\"task:\", task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a7e06",
   "metadata": {},
   "source": [
    "## Step 3 — Dispatch with `ProcessPoolExecutor`, collect results, verify, clean up\n",
    "\n",
    "We try to use a **`fork`** context on Unix (for smoother demos in notebooks).\n",
    "\n",
    "If not available (e.g., Windows), we fall back to the default **spawn** context. \n",
    "\n",
    "We submit tasks, aggregate partial sums, and verify theresult against a single-process NumPy sum. \n",
    "\n",
    "Finally, we **close** and **unlink** the shared memory segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32910627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[progress] 1/24 chunks reduced\n",
      "[progress] 2/24 chunks reduced\n",
      "[progress] 3/24 chunks reduced\n",
      "[progress] 4/24 chunks reduced\n",
      "[progress] 5/24 chunks reduced\n",
      "[progress] 6/24 chunks reduced\n",
      "[progress] 7/24 chunks reduced\n",
      "[progress] 8/24 chunks reduced\n",
      "[progress] 9/24 chunks reduced\n",
      "[progress] 10/24 chunks reduced\n",
      "[progress] 11/24 chunks reduced\n",
      "[progress] 12/24 chunks reduced\n",
      "[progress] 13/24 chunks reduced\n",
      "[progress] 14/24 chunks reduced\n",
      "[progress] 15/24 chunks reduced\n",
      "[progress] 16/24 chunks reduced\n",
      "[progress] 17/24 chunks reduced\n",
      "[progress] 18/24 chunks reduced\n",
      "[progress] 19/24 chunks reduced\n",
      "[progress] 20/24 chunks reduced\n",
      "[progress] 21/24 chunks reduced\n",
      "[progress] 22/24 chunks reduced\n",
      "[progress] 23/24 chunks reduced\n",
      "[progress] 24/24 chunks reduced\n",
      "\n",
      "Parallel sum : -1431.3464050292969\n",
      "Single-proc  : -1431.345703125\n",
      "Abs error    : 0.000701904296875\n",
      "Time check   : 0.02s using 1 worker\n",
      "Time submit : 0.58s using 12 workers\n",
      "✅ Shared memory cleaned up (close + unlink)\n"
     ]
    }
   ],
   "source": [
    "t0_submit = time.perf_counter()\n",
    "total = 0.0\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=n_workers) as ex:\n",
    "    \n",
    "    futures = [ex.submit(chunk_sum, *t) for t in tasks]\n",
    "    \n",
    "    for k, f in enumerate(as_completed(futures), 1):\n",
    "        total += f.result()  # re-raises worker exceptions here\n",
    "        print(f\"[progress] {k}/{len(futures)} chunks reduced\")\n",
    "\n",
    "dt_submit = time.perf_counter() - t0_submit\n",
    "\n",
    "# Verify vs single-process sum\n",
    "t0_check = time.perf_counter()\n",
    "check = float(np.sum(shm_data))\n",
    "dt_check = time.perf_counter() - t0_check\n",
    "\n",
    "print(\"\\nParallel sum :\", total)\n",
    "print(\"Single-proc  :\", check)\n",
    "print(\"Abs error    :\", abs(total - check))\n",
    "print(f\"Time check   : {dt_check:.2f}s using 1 worker\")\n",
    "print(f\"Time submit : {dt_submit:.2f}s using {n_workers} workers\")\n",
    "\n",
    "# Cleanup: close and unlink shared memory\n",
    "try:\n",
    "    shm.close()\n",
    "    shm.unlink()\n",
    "    print(\"✅ Shared memory cleaned up (close + unlink)\")\n",
    "except Exception as e:\n",
    "    print(\"Cleanup note:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460e3cd",
   "metadata": {},
   "source": [
    "# Why `np.sum` (1 worker) beats multiprocessing here\n",
    "\n",
    "**What you measured**\n",
    "- **Single-proc `np.sum` (0.01s):** one tight C loop over contiguous memory → saturates\n",
    "  memory bandwidth, minimal Python overhead.\n",
    "- **Multiprocessing submit (0.10s):** Python orchestrates many processes\n",
    "  + sends tasks,\n",
    "  + attaches to shared memory,\n",
    "  + runs many small reductions,\n",
    "  + collects futures → added overhead\n",
    "  + all processes compete for the same memory bandwidth.\n",
    "\n",
    "**Key reasons**\n",
    "- **Memory-bandwidth bound:** Summing a 76 MiB float32 array is limited by RAM speed, not CPU.\n",
    "  More processes can’t make memory faster; they only add contention.\n",
    "- **Overheads dominate:** Pool startup, pickling small task tuples, `as_completed()` handling,\n",
    "  multiple `np.sum` calls, shared-memory attach/teardown.\n",
    "- **Chunking penalty:** One big C loop is faster than many small ones.\n",
    "\n",
    "**Takeaways**\n",
    "- For pure NumPy reductions/transforms on contiguous arrays → **stay single-process** (or\n",
    "  use a small number of native threads in BLAS/OpenMP if beneficial).\n",
    "- Use **multiprocessing** when the per-element work is **non-vectorizable Python** or heavy\n",
    "  custom code that can’t be expressed as a single NumPy call.\n",
    "\n",
    "\n",
    "# Further reading\n",
    "\n",
    "### What is “contention”?\n",
    "\n",
    "**Definition (in this context):** Multiple workers compete for the **same limited\n",
    "resource** (memory bandwidth, last-level cache, memory controller queues). \n",
    "\n",
    "The total\n",
    "“pipe” isn’t wider, so adding workers doesn’t increase throughput.\n",
    "\n",
    "\n",
    "**What happens when many processes sum the same big array:**\n",
    "- All processes stream from **the same DRAM channels** → the memory subsystem saturates.\n",
    "- **LLC/cache thrash** increases: more evictions, more misses, higher latency.\n",
    "- The OS spends time on **scheduling/context switches** with no net gain.\n",
    "\n",
    "**Symptoms you’ll see:**\n",
    "- More workers ⇒ **no speedup** or even **slower**.\n",
    "- Per-core CPU looks busy, but **wall-time doesn’t drop**.\n",
    "- Perf counters (if you check) show **high memory bandwidth** and **LLC misses**.\n",
    "\n",
    "**Mitigations:**\n",
    "- Prefer **one tight C/NumPy pass** (or a small, tuned thread count) for bandwidth-bound\n",
    "  reductions.\n",
    "- Use **fewer, larger chunks**; avoid many small passes over the same data.\n",
    "- If you must parallelize: split data so workers hit **disjoint memory** (advanced: NUMA\n",
    "  pinning), and keep **threads × processes ≤ cores**.\n",
    "\n",
    "\n",
    "# LLC / Cache Thrash (what & why)\n",
    "\n",
    "**LLC (Last-Level Cache)**: the shared L3 cache for all cores on a socket.  \n",
    "**Cache thrash**: multiple workers evict each other’s cache lines so useful data\n",
    "won’t stay cached → more **LLC misses** → more **DRAM traffic** → slower runs.\n",
    "\n",
    "**Why it happens here**\n",
    "- Many processes stream over large arrays (working set → cache size).  \n",
    "- Shared LLC gets constantly overwritten by neighbors’ streams.  \n",
    "- Net: memory bandwidth saturates; extra workers add contention, not speed.\n",
    "\n",
    "**Symptoms**\n",
    "- High per-core “CPU usage” but little/no wall-time improvement.  \n",
    "- Perf counters: high LLC miss rate, near-peak memory bandwidth.\n",
    "\n",
    "**Mitigations**\n",
    "- Use **fewer workers** (or one tight NumPy/C loop).  \n",
    "- **Block/tile** data so chunks fit better in cache.  \n",
    "- Prefer **one process + small OpenMP thread count (e.g., 2–4)** over many processes.  \n",
    "- On Linux/HPC: **pin** threads/processes and use **NUMA-aware** partitioning\n",
    "  (`OMP_PROC_BIND=close`, `OMP_PLACES=cores`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
