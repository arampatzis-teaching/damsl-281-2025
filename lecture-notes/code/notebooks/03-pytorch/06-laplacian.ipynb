{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_sum(x):\n",
    "    return torch.sum(x**3)\n",
    "\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "\n",
    "\n",
    "full_matrix = torch.func.hessian(f_sum)(x)\n",
    "\n",
    "\n",
    "diagonal = torch.func.vmap(torch.func.grad(torch.func.grad(f_sum)))(x)\n",
    "\n",
    "print(\"--- Method A: Full Hessian (2x2) ---\")\n",
    "print(full_matrix)\n",
    "# Output:\n",
    "# tensor([[ 6.,  0.],   <-- 6 is the derivative for input 1.0\n",
    "#         [ 0., 12.]])  <-- 12 is the derivative for input 2.0\n",
    "\n",
    "print(\"\\n--- Method B: vmap(grad(grad)) (Vector) ---\")\n",
    "print(diagonal)\n",
    "# Output:\n",
    "# tensor([ 6., 12.])\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "# Does Method B match the diagonal of Method A?\n",
    "print(torch.allclose(diagonal, torch.diagonal(full_matrix)))\n",
    "# Output: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ============================================================\n",
    "# 1. SymPy: define u(x, y) and its Laplacian\n",
    "# ============================================================\n",
    "\n",
    "# Symbols\n",
    "x_sym, y_sym = sp.symbols('x y')\n",
    "\n",
    "# Scalar field u(x, y)\n",
    "u_sym = sp.sin(x_sym) * sp.exp(y_sym) + x_sym**2 * y_sym\n",
    "\n",
    "# Laplacian Δu = d²u/dx² + d²u/dy²\n",
    "lap_sym = sp.diff(u_sym, x_sym, 2) + sp.diff(u_sym, y_sym, 2)\n",
    "\n",
    "# Lambdify for numerical evaluation\n",
    "u_fn = sp.lambdify((x_sym, y_sym), u_sym, \"numpy\")\n",
    "lap_fn = sp.lambdify((x_sym, y_sym), lap_sym, \"numpy\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. PyTorch: same u(x, y) and Laplacian via autograd\n",
    "# ============================================================\n",
    "\n",
    "def u_torch(xy: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute u(x, y) = sin(x) * exp(y) + x^2 * y for a batch of points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xy : torch.Tensor\n",
    "        Input tensor of shape (N, 2), where each row contains values [x, y].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Output tensor of shape (N, 1), where each row is u(x, y) evaluated at that point.\n",
    "    \"\"\"\n",
    "    x = xy[:, 0]\n",
    "    y = xy[:, 1]\n",
    "    return torch.sin(x) * torch.exp(y) + x**2 * y\n",
    "\n",
    "def laplacian_autograd(xy):\n",
    "    \"\"\"\n",
    "    Compute the Laplacian Δu of the function u(x, y) via PyTorch autograd.\n",
    "\n",
    "    The Laplacian is calculated as Δu = sum_i d²u/dx_i² for i over spatial variables.\n",
    "    This function performs automatic differentiation to compute the second derivatives\n",
    "    for each spatial coordinate, then sums them to obtain the Laplacian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xy : torch.Tensor\n",
    "        Input tensor of shape (N, 2), where N is the number of points (batch size).\n",
    "        Each row represents the coordinates [x, y] of a point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Output tensor of shape (N, 1), where each row contains the Laplacian\n",
    "        evaluated at the corresponding input point.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The Laplacian is computed by summing the second partial derivatives\n",
    "    with respect to all input coordinates for each point in the batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # detach() breaks the computational graph and makes xy a leaf node\n",
    "    # requires_grad_(True) makes it a leaf node and track its gradient\n",
    "    xy = xy.detach().requires_grad_(True)\n",
    "    u = u_torch(xy)              # (N, 1)\n",
    "\n",
    "    # First derivatives wrt x,y\n",
    "    grads = torch.autograd.grad(\n",
    "        u.sum(), xy,\n",
    "        create_graph=True\n",
    "    )[0]                          # (N, 2)\n",
    "\n",
    "    lap = 0.0\n",
    "    for i in range(xy.shape[1]):\n",
    "        # d²u/dx_i² = d/dx_i (du/dx_i)\n",
    "        gi = grads[:, i:i+1]      # (N, 1)\n",
    "        g2 = torch.autograd.grad(\n",
    "            gi.sum(), xy,\n",
    "            create_graph=True\n",
    "        )[0][:, i:i+1]            # (N, 1)\n",
    "        lap = lap + g2\n",
    "\n",
    "    return lap                    # (N, 1)\n",
    "\n",
    "# ============================================================\n",
    "# 3. Compare SymPy vs PyTorch on random points\n",
    "# ============================================================\n",
    "\n",
    "# Use double precision for a better comparison\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Sample N points in R^2\n",
    "N = 5\n",
    "xy_torch = torch.randn(N, 2, requires_grad=True)\n",
    "xy_np = xy_torch.detach().numpy()\n",
    "xs, ys = xy_np[:, 0], xy_np[:, 1]\n",
    "\n",
    "# SymPy Laplacian at those points\n",
    "lap_sym_np = lap_fn(xs, ys)                # shape (N,)\n",
    "lap_sym_torch = torch.from_numpy(\n",
    "    np.asarray(lap_sym_np)\n",
    ").reshape(N, 1)\n",
    "\n",
    "# PyTorch autograd Laplacian\n",
    "lap_torch = laplacian_autograd(xy_torch)   # (N, 1)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Print results\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nPoints (x, y):\")\n",
    "print(xy_torch)\n",
    "\n",
    "print(\"\\nLaplacian from SymPy:\")\n",
    "print(lap_sym_torch)\n",
    "\n",
    "print(\"\\nLaplacian from PyTorch autograd:\")\n",
    "print(lap_torch)\n",
    "\n",
    "abs_diff = (lap_torch - lap_sym_torch).abs()\n",
    "print(\"\\nAbsolute differences:\")\n",
    "print(abs_diff)\n",
    "\n",
    "print(\"\\nMax |difference| =\", abs_diff.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033961ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import func as F  # PyTorch 2.x\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "def u_torch(xy):\n",
    "    # xy: (..., d)\n",
    "    x = xy[..., 0]\n",
    "    y = xy[..., 1]\n",
    "    return torch.sin(x) * torch.exp(y) + x**2 * y  # returns (...,)\n",
    "\n",
    "def laplacian_exact(xy):\n",
    "    \"\"\"\n",
    "    xy: (N, d) with requires_grad=True\n",
    "    returns: (N, 1) Laplacian\n",
    "    \"\"\"\n",
    "\n",
    "    def scalar_u(x):\n",
    "        # x: (d,)\n",
    "        return u_torch(x.unsqueeze(0))[0]  # scalar\n",
    "\n",
    "    # Hessian per point: (d, d)\n",
    "    hess_fn = F.hessian(scalar_u)\n",
    "\n",
    "    # vmap over batch: xy: (N, d) → hess: (N, d, d)\n",
    "    H = F.vmap(hess_fn)(xy)\n",
    "\n",
    "    # trace over last two dims → (N,)\n",
    "    lap = H.diagonal(dim1=-2, dim2=-1).sum(-1)\n",
    "    return lap.unsqueeze(-1)\n",
    "\n",
    "N, d = 5, 2\n",
    "xy = torch.randn(N, d, requires_grad=True)\n",
    "lap = laplacian_exact(xy)\n",
    "\n",
    "lap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
