{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c275fc10",
   "metadata": {},
   "source": [
    "### `torch.autograd.functional.vjp`\n",
    "\n",
    "This notebook demonstrates how to use the `torch.autograd.functional.vjp` function \n",
    "to compute the vector-Jacobian product (VJP) of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b867e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2870b",
   "metadata": {},
   "source": [
    "### Scalar function example\n",
    "\n",
    "This script computes derivatives using `torch.autograd.functional.vjp`, which calculates the product of a vector $v$ with the Jacobian matrix of the function.\n",
    "\n",
    "* **`v = torch.ones_like(x)`**: Defines the vector $v$. In the chain rule, this acts as the \"incoming gradient.\" Setting it to all ones preserves the exact gradient of $f(x)$.\n",
    "* **`vjp(f, x, v=v)`**: Computes both the output and the gradients in a single efficient step:\n",
    "    1.  **`y`**: The function output ($f(x)$).\n",
    "    2.  **`dy_dx`**: The Vector-Jacobian Product ($v^T \\cdot J$).\n",
    "* **Result**: Since $v$ is 1, this effectively returns the standard element-wise derivatives for the batch without explicitly constructing a large Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650dc66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are derivatives correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x**2 + torch.sin(x**2)\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 2*x + 2*x*torch.cos(x**2)\n",
    "\n",
    "N = 1000\n",
    "x = torch.linspace(0, 5, N, requires_grad=True)\n",
    "\n",
    "# v must have the same shape as f(x)\n",
    "v = torch.ones_like(x)\n",
    "\n",
    "# For vector-valued f, pass v directly\n",
    "y, dy_dx = torch.autograd.functional.vjp(f, x, v=v)\n",
    "\n",
    "print(\"Are derivatives correct?\", torch.allclose(df_dx(x), dy_dx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348844b6",
   "metadata": {},
   "source": [
    "### Vector-valued function example\n",
    "\n",
    "This script calculates gradients for a function that outputs a list of results (one for each sample in the batch).\n",
    "\n",
    "* **`f(x)`**: Takes a batch of inputs `(N, 2)` and returns a batch of scalar outputs `(N,)`.\n",
    "* **`v = torch.ones(N)`**: The \"incoming gradient\" vector. It must match the output shape of `f(x)`. Setting it to ones preserves the gradients exactly.\n",
    "* **`vjp(f, x, v=v)`**: Computes the Vector-Jacobian Product.\n",
    "    * Since `f` outputs a vector (the batch results), we provide `v` to weight these outputs during backpropagation.\n",
    "    * It calculates the derivative of the outputs with respect to the inputs `x`.\n",
    "    * **Result**: `grad_x_vjp` (shape `(N, 2)`) contains the partial derivatives for every sample in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8396a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are gradients correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x[:, 0] * x[:, 1] + torch.sin(x[:, 0] * x[:, 1]**2)\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.stack(\n",
    "        (\n",
    "            x[:, 1] + x[:, 1]**2 * torch.cos(x[:, 0] * x[:, 1]**2),\n",
    "            x[:, 0] + 2 * x[:, 0] * x[:, 1] * torch.cos(x[:, 0] * x[:, 1]**2),\n",
    "        ),\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = 2 * torch.rand(N, 2) - 1\n",
    "x.requires_grad = True\n",
    "\n",
    "# v must have same shape as f(x), i.e. (N,)\n",
    "v = torch.ones(N)\n",
    "\n",
    "# vjp for vector output: pass v directly\n",
    "y, grad_x_vjp = torch.autograd.functional.vjp(f, x, v=v)\n",
    "\n",
    "dfdx = df_dx(x)\n",
    "\n",
    "print(\"Are gradients correct?\", torch.allclose(dfdx, grad_x_vjp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c409c041",
   "metadata": {},
   "source": [
    "### Vector-valued function with multiple inputs\n",
    "\n",
    "This script demonstrates how to compute gradients for a function that takes **multiple independent arguments** ($x$ and $y$).\n",
    "\n",
    "* **`f(x, y)`**: A function accepting two separate input tensors.\n",
    "* **`vjp(f, (x, y), v=v)`**: Computes the Vector-Jacobian Product for multiple inputs simultaneously.\n",
    "    * **`inputs=(x, y)`**: By passing a tuple of inputs, `vjp` tracks gradients for both tensors.\n",
    "    * **Result**: It returns a tuple `(vjp_x, vjp_y)`, where `vjp_x` is the partial derivative with respect to $x$, and `vjp_y` is the partial derivative with respect to $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cf31ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx correct? True\n",
      "dy correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x*y + torch.sin(x*y**2)\n",
    "\n",
    "def df_dx(x, y):\n",
    "    return torch.stack(\n",
    "        (\n",
    "            y + y**2 * torch.cos(x*y**2),\n",
    "            x + 2*x*y * torch.cos(x*y**2),\n",
    "        ),\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = (2*torch.rand(N) - 1).requires_grad_()\n",
    "y = (2*torch.rand(N) - 1).requires_grad_()\n",
    "\n",
    "# v must match f(x,y) shape (N,)\n",
    "v = torch.ones(N)\n",
    "\n",
    "# vjp with multiple inputs: inputs=(x,y)\n",
    "out, (vjp_x, vjp_y) = torch.autograd.functional.vjp(\n",
    "    f,\n",
    "    (x, y),\n",
    "    v=v\n",
    ")\n",
    "\n",
    "J = df_dx(x, y)\n",
    "\n",
    "print(\"dx correct?\", torch.allclose(J[:, 0], vjp_x))\n",
    "print(\"dy correct?\", torch.allclose(J[:, 1], vjp_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653d333",
   "metadata": {},
   "source": [
    "### Vector-valued function with multiple inputs\n",
    "\n",
    "This script reconstructs the full Jacobian matrix by iterating through the output dimensions.\n",
    "\n",
    "* **The Problem**: `vjp` computes a single vector (a weighted sum of gradients), not the full Jacobian matrix.\n",
    "* **The Loop**: To get the full Jacobian, we must isolate each output component ($y_1, y_2, y_3$) individually.\n",
    "* **`v[:, k] = 1.0`**: Inside the loop, we create a \"one-hot\" vector. This acts as a selector. When passed to `vjp`, it forces PyTorch to calculate the gradient of *only* the $k$-th output variable.\n",
    "* **Result**: By running `vjp` 3 times and placing the results into `Js[:, k, :]`, we build the complete Jacobian matrix row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are Jacobians correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "\n",
    "    y1 = x1 * x2\n",
    "    y2 = torch.sin(x1 + x2**2)\n",
    "    y3 = x1**2 - 3 * x2\n",
    "\n",
    "    return torch.stack((y1, y2, y3), dim=1)   # (N, 3)\n",
    "\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "\n",
    "    dy1_dx1 = x2\n",
    "    dy1_dx2 = x1\n",
    "\n",
    "    dy2_dx1 = torch.cos(x1 + x2**2)\n",
    "    dy2_dx2 = 2 * x2 * torch.cos(x1 + x2**2)\n",
    "\n",
    "    dy3_dx1 = 2 * x1\n",
    "    dy3_dx2 = -3 * torch.ones_like(x2)\n",
    "\n",
    "    J = torch.stack(\n",
    "        (\n",
    "            torch.stack((dy1_dx1, dy1_dx2), dim=1),\n",
    "            torch.stack((dy2_dx1, dy2_dx2), dim=1),\n",
    "            torch.stack((dy3_dx1, dy3_dx2), dim=1),\n",
    "        ),\n",
    "        dim=1\n",
    "    )   # (N, 3, 2)\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = (2 * torch.rand(N, 2) - 1).requires_grad_()\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "# ----------------------- vjp version -----------------------\n",
    "Js = torch.zeros(N, 3, 2)\n",
    "\n",
    "for k in range(3):     # loop over output dims\n",
    "    v = torch.zeros_like(y)   # shape (N, 3)\n",
    "    v[:, k] = 1.0             # pick k-th output component\n",
    "\n",
    "    _, grad_x = torch.autograd.functional.vjp(\n",
    "        f,\n",
    "        x,\n",
    "        v=v\n",
    "    )\n",
    "\n",
    "    Js[:, k, :] = grad_x      # grad_x is (N, 2)\n",
    "\n",
    "\n",
    "dfdx = df_dx(x)\n",
    "\n",
    "print(\"Are Jacobians correct?\", torch.allclose(dfdx, Js))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d889d",
   "metadata": {},
   "source": [
    "### Code Description: Multi-Input Jacobian via VJP Loop\n",
    "\n",
    "This script reconstructs the full Jacobian matrix for a function taking **two independent inputs** ($x, y$) and producing **three outputs**.\n",
    "\n",
    "* **The Loop**: To get the full Jacobian matrix, we must iterate through the 3 output dimensions ($y_1, y_2, y_3$).\n",
    "* **`v[:, k] = 1.0`**: The \"selector\" vector. By setting only the $k$-th column to 1, we isolate the gradient calculation for that specific output equation.\n",
    "* **`vjp(f, (x, y), v=v)`**:\n",
    "    * Accepts a **tuple** of inputs `(x, y)`.\n",
    "    * Returns a **tuple** of gradients `(gx, gy)`.\n",
    "* **Reconstruction**: We manually fill the Jacobian tensor `Js` row by row:\n",
    "    * `Js[:, k, 0] = gx`: Stores partial derivatives w.r.t. $x$.\n",
    "    * `Js[:, k, 1] = gy`: Stores partial derivatives w.r.t. $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3363589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    y1 = x * y\n",
    "    y2 = torch.sin(x + y**2)\n",
    "    y3 = x**2 - 3*y\n",
    "    return torch.stack((y1, y2, y3), dim=1)   # (N, 3)\n",
    "\n",
    "\n",
    "def df_dxdy(x, y):\n",
    "    dy1_dx = y\n",
    "    dy1_dy = x\n",
    "\n",
    "    dy2_dx = torch.cos(x + y**2)\n",
    "    dy2_dy = 2*y * torch.cos(x + y**2)\n",
    "\n",
    "    dy3_dx = 2*x\n",
    "    dy3_dy = -3 * torch.ones_like(y)\n",
    "\n",
    "    J = torch.stack(\n",
    "        (\n",
    "            torch.stack((dy1_dx, dy1_dy), dim=1),\n",
    "            torch.stack((dy2_dx, dy2_dy), dim=1),\n",
    "            torch.stack((dy3_dx, dy3_dy), dim=1),\n",
    "        ),\n",
    "        dim=1\n",
    "    )   # (N, 3, 2)\n",
    "    return J\n",
    "\n",
    "\n",
    "N = 1000\n",
    "x = (2*torch.rand(N) - 1).requires_grad_()\n",
    "y = (2*torch.rand(N) - 1).requires_grad_()\n",
    "\n",
    "out = f(x, y)      # shape (N, 3)\n",
    "\n",
    "Js = torch.zeros(N, 3, 2)\n",
    "\n",
    "for k in range(3):\n",
    "    # v has same shape as out: (N, 3)\n",
    "    v = torch.zeros_like(out)\n",
    "    v[:, k] = 1.0   # pick k-th output component\n",
    "\n",
    "    # vjp with multiple inputs: returns (out, (vjp_wrt_x, vjp_wrt_y))\n",
    "    _, (gx, gy) = torch.autograd.functional.vjp(\n",
    "        f,\n",
    "        (x, y),\n",
    "        v=v,\n",
    "    )\n",
    "\n",
    "    Js[:, k, 0] = gx\n",
    "    Js[:, k, 1] = gy\n",
    "\n",
    "J_true = df_dxdy(x, y)\n",
    "\n",
    "print(\"Correct?\", torch.allclose(Js, J_true))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
