{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089da7d4",
   "metadata": {},
   "source": [
    "This notebook demonstrates:\n",
    "\n",
    "- The use of PyTorch for automatic differentiation.\n",
    "- How to compute gradients of functions using `torch.autograd`.\n",
    "- Verification of manually derived gradients against those computed via autograd.\n",
    "- Working with multi-dimensional tensors in gradient computations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aca2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c60b2",
   "metadata": {},
   "source": [
    "### Scalar function example\n",
    "\n",
    "Use `torch.autograd.grad` to compute the gradient of a scalar function in a batch.\n",
    "\n",
    "The `grad_outputs` argument should be set to `torch.ones_like(y)` in order to compute the\n",
    "vector-gradient product of the seed vector $( 1, \\ldots, 1 )^\\top$ \n",
    "with the gradient of the function $( f(x_1), \\ldots, f(x_N) )^\\top$.\n",
    "\n",
    "Since the Jacobian is diagonal, the $i$-th element of the vector-gradient product is\n",
    "$\\dfrac{\\partial f}{\\partial x_i}(x_i)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c3330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are gradients correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x[:,0]*x[:,1] + torch.sin(x[:,0]*x[:,1]**2)\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.stack(\n",
    "        (\n",
    "            x[:,1] + x[:,1]**2*torch.cos(x[:,0]*x[:,1]**2), \n",
    "            x[:,0] + 2*x[:,0]*x[:,1]*torch.cos(x[:,0]*x[:,1]**2)\n",
    "        ),\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = 2 * torch.rand(N, 2) - 1\n",
    "x.requires_grad = True\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "# this is equivalent to y.backward(ones)\n",
    "grad_x = torch.autograd.grad(\n",
    "    outputs=y,\n",
    "    inputs=x,\n",
    "    grad_outputs=torch.ones_like(y)\n",
    ")[0]\n",
    "\n",
    "dfdx = df_dx(x)\n",
    "\n",
    "print(\"Are gradients correct?\", torch.allclose(dfdx, grad_x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67364ca7",
   "metadata": {},
   "source": [
    "### Vector function example\n",
    "\n",
    "Use `torch.autograd.grad` to compute the Jacobian of a vector function in a batch.\n",
    "\n",
    "Use two tensors $x$ and $y$ to compute the Jacobian of the vector function $f(x,y)$.\n",
    "\n",
    "This is important in order to accumulate the derivative of $f$ with respect to $x$ and $y$\n",
    "to the tensor $x$ and $y$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12b0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx correct? True\n",
      "dy correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    return x*y + torch.sin(x*y**2)\n",
    "\n",
    "def df_dx(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.stack(\n",
    "        (\n",
    "            y + y**2 * torch.cos(x*y**2),\n",
    "            x + 2*x*y * torch.cos(x*y**2)\n",
    "        ),\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = (2*torch.rand(N) - 1).requires_grad_()\n",
    "y = (2*torch.rand(N) - 1).requires_grad_()\n",
    "\n",
    "out = f(x, y)\n",
    "\n",
    "\n",
    "grad_x, grad_y = torch.autograd.grad(\n",
    "    outputs=out,\n",
    "    inputs=(x, y),\n",
    "    grad_outputs=torch.ones_like(out)\n",
    ")\n",
    "\n",
    "J = df_dx(x, y)\n",
    "\n",
    "print(\"dx correct?\", torch.allclose(J[:,0], grad_x))\n",
    "print(\"dy correct?\", torch.allclose(J[:,1], grad_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954fe6e9",
   "metadata": {},
   "source": [
    "### Jacobian example\n",
    "\n",
    "Use `torch.autograd.grad` to compute the Jacobian of a vector function in a batch.\n",
    "\n",
    "In order to compute the Jacobian, we loop over the output dimensions and:\n",
    "\n",
    "1. set the corresponding element of the `gradient` tensor to one and the rest to zero,\n",
    "2. set `retain_graph=True` to allow the reuse of intermediate results for the next iteration,\n",
    "otherwise the computational graph will be deleted after the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9475e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: torch.Size([1000, 3, 2]) torch.Size([1000, 3, 2])\n",
      "Are Jacobians correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "\n",
    "    y1 = x1 * x2\n",
    "    y2 = torch.sin(x1 + x2**2)\n",
    "    y3 = x1**2 - 3 * x2\n",
    "\n",
    "    return torch.stack((y1, y2, y3), dim=1)\n",
    "\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "\n",
    "    dy1_dx1 = x2\n",
    "    dy1_dx2 = x1\n",
    "\n",
    "    dy2_dx1 = torch.cos(x1 + x2**2)\n",
    "    dy2_dx2 = 2 * x2 * torch.cos(x1 + x2**2)\n",
    "\n",
    "    dy3_dx1 = 2 * x1\n",
    "    dy3_dx2 = -3 * torch.ones_like(x2)\n",
    "\n",
    "    J = torch.stack(\n",
    "        (\n",
    "            torch.stack((dy1_dx1, dy1_dx2), dim=1),\n",
    "            torch.stack((dy2_dx1, dy2_dx2), dim=1),\n",
    "            torch.stack((dy3_dx1, dy3_dx2), dim=1),\n",
    "        ),\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = (2 * torch.rand(N, 2) - 1).requires_grad_()\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "# ---- Jacobian using autograd.grad instead of backward ----\n",
    "Js = torch.zeros(N, 3, 2)\n",
    "\n",
    "for k in range(3):     # 3 outputs\n",
    "    g = torch.zeros_like(y) \n",
    "    g[:, k] = 1.0\n",
    "    grad_x = torch.autograd.grad(\n",
    "        outputs=y,\n",
    "        inputs=x,\n",
    "        grad_outputs=g,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    Js[:, k, :] = grad_x\n",
    "\n",
    "\n",
    "dfdx = df_dx(x)\n",
    "\n",
    "print(\"Shapes:\", dfdx.shape, Js.shape)\n",
    "print(\"Are Jacobians correct?\", torch.allclose(dfdx, Js))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a74777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct? True\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    y1 = x * y\n",
    "    y2 = torch.sin(x + y**2)\n",
    "    y3 = x**2 - 3*y\n",
    "    return torch.stack((y1, y2, y3), dim=1)\n",
    "\n",
    "\n",
    "def df_dxdy(x, y):\n",
    "    dy1_dx = y\n",
    "    dy1_dy = x\n",
    "\n",
    "    dy2_dx = torch.cos(x + y**2)\n",
    "    dy2_dy = 2*y * torch.cos(x + y**2)\n",
    "\n",
    "    dy3_dx = 2*x\n",
    "    dy3_dy = -3 * torch.ones_like(y)\n",
    "\n",
    "    J = torch.stack(\n",
    "        (\n",
    "            torch.stack((dy1_dx, dy1_dy), dim=1),\n",
    "            torch.stack((dy2_dx, dy2_dy), dim=1),\n",
    "            torch.stack((dy3_dx, dy3_dy), dim=1),\n",
    "        ),\n",
    "        dim=1\n",
    "    )\n",
    "    return J\n",
    "\n",
    "\n",
    "N = 1000\n",
    "x = (2*torch.rand(N) - 1).requires_grad_()\n",
    "y = (2*torch.rand(N) - 1).requires_grad_()\n",
    "\n",
    "out = f(x, y)      # shape [N, 3]\n",
    "\n",
    "Js = torch.zeros(N, 3, 2)\n",
    "\n",
    "for k in range(3):\n",
    "    g = torch.zeros_like(out)\n",
    "    g[:, k] = 1.0\n",
    "    gx, gy = torch.autograd.grad(out, (x, y), g, retain_graph=True)\n",
    "    Js[:, k, 0] = gx\n",
    "    Js[:, k, 1] = gy\n",
    "\n",
    "J_true = df_dxdy(x, y)\n",
    "\n",
    "print(\"Correct?\", torch.allclose(Js, J_true))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad38b97",
   "metadata": {},
   "source": [
    "# Practical differences between `torch.autograd.grad` and `.backward()`\n",
    "\n",
    "## ✅ Benefits of `torch.autograd.grad` over `.backward()`\n",
    "\n",
    "### 1. `autograd.grad` RETURNS the gradient — it does NOT write into `.grad`\n",
    "\n",
    "```python\n",
    "gx = torch.autograd.grad(y, x)[0]\n",
    "```\n",
    "\n",
    "vs.\n",
    "\n",
    "```python\n",
    "y.backward()\n",
    "print(x.grad)   # must read from .grad\n",
    "```\n",
    "\n",
    "With `.backward()`, you must manage:\n",
    "\n",
    "- `x.grad` accumulation  \n",
    "- resetting: `x.grad = None` or `x.grad.zero_()`\n",
    "\n",
    "With `autograd.grad`, no accumulation ever happens.  \n",
    "The gradient is clean, isolated, and directly usable.\n",
    "\n",
    "➡️ Cleaner and safer.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `autograd.grad` works naturally with multiple inputs\n",
    "\n",
    "```python\n",
    "gx, gy = torch.autograd.grad(out, (x, y))\n",
    "```\n",
    "\n",
    "`.backward()` cannot return multiple gradients.  \n",
    "It only fills `.grad` fields.\n",
    "\n",
    "You must manually read `x.grad`, `y.grad`, reset them, etc.\n",
    "\n",
    "➡️ Better when you have multiple inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `autograd.grad` is necessary when you want Jacobian rows without side effects\n",
    "\n",
    "To compute a Jacobian row cleanly:\n",
    "\n",
    "```python\n",
    "gx = torch.autograd.grad(y[:, k], x)[0]\n",
    "```\n",
    "\n",
    "No need to clear `.grad`.\n",
    "\n",
    "Using `.backward()`:\n",
    "\n",
    "```python\n",
    "y[:, k].backward()\n",
    "J[k] = x.grad\n",
    "x.grad = None\n",
    "```\n",
    "\n",
    "You must reset every time.\n",
    "\n",
    "➡️ `autograd.grad` is cleaner: no state, no mutation.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `autograd.grad` is functional; `.backward()` is stateful\n",
    "\n",
    "Functional = predictable, easy to reason about  \n",
    "Stateful = `.grad` persists or accumulates\n",
    "\n",
    "➡️ In analytical gradient checks and research code, functional is preferred.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `autograd.grad` is safer inside loops\n",
    "\n",
    "Inside loops:\n",
    "\n",
    "- `.backward()` accumulates — you must reset manually  \n",
    "- `autograd.grad` returns fresh gradients — no accumulation ever\n",
    "\n",
    "➡️ Fewer bugs.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. `autograd.grad` is better for custom loss constructions\n",
    "\n",
    "You can build arbitrary VJPs:\n",
    "\n",
    "```python\n",
    "vjp = torch.autograd.grad(y, x, grad_outputs=v)[0]\n",
    "```\n",
    "\n",
    "`.backward()` forces writing into `.grad`, which can conflict with training loops.\n",
    "\n",
    "➡️ Essential for advanced differentiable programming.\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ When `.backward()` is better\n",
    "\n",
    "`.backward()` is the right choice when you are **training a model with an optimizer**.\n",
    "\n",
    "Why?\n",
    "\n",
    "### 1. Optimizers expect gradients to be in `param.grad`\n",
    "PyTorch optimizers (SGD, Adam, RMSProp, etc.) read gradients **directly** from  \n",
    "each parameter’s `.grad` field:\n",
    "\n",
    "```python\n",
    "loss.backward()    # fills p.grad for every parameter\n",
    "optimizer.step()   # optimizer uses p.grad internally\n",
    "```\n",
    "\n",
    "`torch.autograd.grad` does NOT fill `param.grad`.  \n",
    "You would have to assign every gradient manually.  \n",
    "This is error-prone and completely unnecessary.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `.backward()` handles ALL parameters automatically\n",
    "A neural network can have thousands/millions of parameters.  \n",
    "`.backward()` computes and stores gradients for **all of them at once**.\n",
    "\n",
    "With `torch.autograd.grad`, you would need:\n",
    "\n",
    "```python\n",
    "grads = torch.autograd.grad(loss, model.parameters())\n",
    "for p, g in zip(model.parameters(), grads):\n",
    "    p.grad = g     # manual wiring\n",
    "```\n",
    "\n",
    "`.backward()` does this wiring for you.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `.backward()` supports gradient accumulation\n",
    "Training often uses mini-batches or multi-step accumulation:\n",
    "\n",
    "```python\n",
    "loss.backward()   # adds to existing p.grad\n",
    "loss.backward()   # adds again\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "`autograd.grad` NEVER accumulates.  \n",
    "It always returns a fresh gradient and discards it unless you store it manually.\n",
    "\n",
    "Gradient accumulation is essential for:\n",
    "\n",
    "- multi-GPU training  \n",
    "- large batch emulation  \n",
    "- gradient checkpointing  \n",
    "- truncated BPTT  \n",
    "\n",
    "---\n",
    "\n",
    "### 4. `.backward()` integrates with the entire PyTorch training ecosystem\n",
    "All PyTorch tools assume `.backward()` is building gradients:\n",
    "\n",
    "- optimizers  \n",
    "- schedulers  \n",
    "- AMP (automatic mixed precision)  \n",
    "- DDP (DistributedDataParallel)  \n",
    "- hooks on `.grad`  \n",
    "- gradient clipping  \n",
    "\n",
    "Using `autograd.grad` bypasses most of this infrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `.backward()` is designed for the standard training loop\n",
    "It matches the classic workflow:\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "This pattern is universal, stable, and deeply integrated in PyTorch’s design.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Use `.backward()` for **model training**, because only `.backward()`:\n",
    "\n",
    "- fills `param.grad` automatically  \n",
    "- accumulates gradients correctly  \n",
    "- integrates with optimizers  \n",
    "- supports AMP, DDP, schedulers, clipping, hooks  \n",
    "- scales to large models with millions of parameters  \n",
    "\n",
    "`torch.autograd.grad` is powerful and precise, but it is **not** a replacement  \n",
    "for `.backward()` in training loops.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1e185d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m grad_x, grad_y = torch.autograd.grad( outputs=out, inputs=(x, y), grad_outputs=torch.ones_like(out) )\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m grad_xx, = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/UOC-work/teaching/2025/02-SciML/public-repo/lecture-notes/code/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:411\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    407\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    408\u001b[39m         grad_outputs_\n\u001b[32m    409\u001b[39m     )\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     result = \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    422\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    423\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    424\u001b[39m     ):\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "grad_x, grad_y = torch.autograd.grad( outputs=out, inputs=(x, y), grad_outputs=torch.ones_like(out) )\n",
    "grad_xx, = torch.autograd.grad( outputs=grad_x, inputs=(x, ), grad_outputs=torch.ones_like(grad_x) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
