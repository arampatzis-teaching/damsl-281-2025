{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f92be8",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f7ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.func import jvp\n",
    "from torch.func import jacfwd, jacrev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffa8bf",
   "metadata": {},
   "source": [
    "## Symbolic vs Automatic Differentiation\n",
    "\n",
    "This example compares analytical and automatic differentiation on the same\n",
    "function. \n",
    "\n",
    "### First Derivative\n",
    "Using `SymPy`, the derivative is computed exactly and then converted\n",
    "into a numerical callable. Using `PyTorch`, the derivative is obtained through\n",
    "autograd by creating inputs with gradient tracking and calling `backward()` to\n",
    "extract the gradient.\n",
    "\n",
    "Both approaches are evaluated over a uniform grid and plotted together,\n",
    "highlighting how closely `PyTorch`’s autograd matches the exact derivative\n",
    "computed with `SymPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Symbolic derivative with sympy\n",
    "# ---------------------------------------------------------\n",
    "x = sp.symbols('x')\n",
    "f_expr = sp.sin(x) * sp.exp(-x/3) + x**2 * sp.cos(2*x)\n",
    "df_expr = sp.diff(f_expr, x)\n",
    "\n",
    "# convert sympy expr → numeric function\n",
    "f_sym = sp.lambdify(x, f_expr, \"numpy\")\n",
    "df_sym = sp.lambdify(x, df_expr, \"numpy\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Autograd derivative with PyTorch\n",
    "# ---------------------------------------------------------\n",
    "def f_torch(x):\n",
    "    return torch.sin(x) * torch.exp(-x/3) + x**2 * torch.cos(2 * x)\n",
    "\n",
    "def df_torch(x):\n",
    "    grads = []\n",
    "    for xv in x:\n",
    "        xv_t = torch.tensor(xv, dtype=torch.float32, requires_grad=True)\n",
    "        y = f_torch(xv_t)\n",
    "        y.backward()\n",
    "        grads.append(xv_t.grad.item())\n",
    "    return np.array(grads)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Compare on a grid\n",
    "# ---------------------------------------------------------\n",
    "xs = np.linspace(-4, 4, 400)\n",
    "df_sym_vals = df_sym(xs)\n",
    "df_autograd_vals = df_torch(xs)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Plot\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(xs, df_sym_vals, label=\"Symbolic derivative (sympy)\", linewidth=2)\n",
    "plt.plot(xs, df_autograd_vals, \"--\", label=\"Autograd derivative (PyTorch)\")\n",
    "plt.title(\"Comparison of Symbolic vs Autograd Derivatives\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"df/dx\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e5f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2f_expr = sp.diff(df_expr, x)\n",
    "\n",
    "# convert to numpy-callable functions\n",
    "d2f_sym = sp.lambdify(x, d2f_expr, \"numpy\")\n",
    "\n",
    "def d2f_torch(x_values):\n",
    "    grads2 = []\n",
    "    for xv in x_values:\n",
    "        xv_t = torch.tensor(xv, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # First derivative\n",
    "        y = f_torch(xv_t)\n",
    "        dy_dx = torch.autograd.grad(y, xv_t, create_graph=True)[0]\n",
    "\n",
    "        # Second derivative\n",
    "        d2y_dx2 = torch.autograd.grad(dy_dx, xv_t)[0]\n",
    "\n",
    "        grads2.append(d2y_dx2.item())\n",
    "    return np.array(grads2)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Compare on a grid\n",
    "# ---------------------------------------------------------\n",
    "xs = np.linspace(-4, 4, 400)\n",
    "d2f_sym_vals = d2f_sym(xs)\n",
    "d2f_autograd_vals = d2f_torch(xs)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Plot\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(xs, d2f_sym_vals, label=\"Symbolic second derivative (sympy)\", linewidth=2)\n",
    "plt.plot(xs, d2f_autograd_vals, \"--\", label=\"Autograd second derivative (PyTorch)\")\n",
    "plt.title(\"Comparison of Symbolic vs Autograd Second Derivatives\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"d²f/dx²\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359c009",
   "metadata": {},
   "source": [
    "### Second Derivative\n",
    "\n",
    "This example compares two approaches for computing the derivative of the same\n",
    "function. Using `SymPy`, the derivative is obtained analytically and converted\n",
    "into a numerical function for fast evaluation. Using `PyTorch`, forward-mode\n",
    "automatic differentiation is applied via `jvp`, where each input is paired with\n",
    "a tangent value of `1.0` to produce the forward derivative directly.\n",
    "\n",
    "Both methods are evaluated on a dense grid and plotted together, illustrating\n",
    "how forward-mode AD from `PyTorch` aligns with the exact symbolic derivative\n",
    "computed by `SymPy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aafe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Symbolic derivative with sympy\n",
    "# ---------------------------------------------------------\n",
    "x = sp.symbols('x')\n",
    "f_expr = sp.sin(x) * sp.exp(-x/3) + x**2 * sp.cos(2*x)\n",
    "df_expr = sp.diff(f_expr, x)\n",
    "\n",
    "df_sym = sp.lambdify(x, df_expr, \"numpy\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Forward-mode AD with PyTorch (jvp)\n",
    "# ---------------------------------------------------------\n",
    "def f_torch(x_t):\n",
    "    return torch.sin(x_t) * torch.exp(-x_t/3) + x_t**2 * torch.cos(2 * x_t)\n",
    "\n",
    "def df_forward(x_values):\n",
    "    grads = []\n",
    "    for xv in x_values:\n",
    "        # primal value (x) and tangent (dx/dx = 1)\n",
    "        primals = (torch.tensor(xv, dtype=torch.float32),)\n",
    "        tangents = (torch.tensor(1.0, dtype=torch.float32),)\n",
    "\n",
    "        y, dydx = jvp(f_torch, primals, tangents)\n",
    "        grads.append(dydx.item())\n",
    "    return np.array(grads)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Compare on a grid\n",
    "# ---------------------------------------------------------\n",
    "xs = np.linspace(-4, 4, 400)\n",
    "df_sym_vals = df_sym(xs)\n",
    "df_fwd_vals = df_forward(xs)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Plot\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(xs, df_sym_vals, label=\"Symbolic derivative (sympy)\", linewidth=2)\n",
    "plt.plot(xs, df_fwd_vals, \"--\", label=\"Forward-mode AD (PyTorch jvp)\")\n",
    "plt.title(\"Symbolic vs Forward-mode AD Derivative\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"df/dx\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d64064",
   "metadata": {},
   "source": [
    "## Forward-mode vs Reverse-mode Jacobians\n",
    "\n",
    "This example computes the Jacobian of a vector-valued function  \n",
    "$F: \\mathbb{R}^3 \\to \\mathbb{R}^2$ using two automatic-differentiation strategies in `PyTorch`.\n",
    "Forward-mode (`jacfwd`) propagates tangents from inputs to outputs, making it\n",
    "efficient when the input dimension is small. Reverse-mode (`jacrev`) propagates\n",
    "sensitivities from outputs back to inputs, which is advantageous when the output\n",
    "dimension is small.\n",
    "\n",
    "Both Jacobians are computed at a single point and compared element-wise. The\n",
    "results confirm that `jacfwd` and `jacrev` produce the same Jacobian (up to\n",
    "numerical precision), illustrating the consistency of the two differentiation\n",
    "modes for this function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61486379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F: R^3 -> R^2\n",
    "def F(x):\n",
    "    # x: shape (3,)\n",
    "    x0, x1, x2 = x\n",
    "    y1 = torch.sin(x0) + x1**2\n",
    "    y2 = torch.exp(x2) - x0 * x1\n",
    "    return torch.stack([y1, y2])   # shape (2,)\n",
    "\n",
    "# single input point in R^3\n",
    "x = torch.tensor([0.3, -1.2, 0.7], dtype=torch.float32)\n",
    "\n",
    "# Forward-mode Jacobian (jacfwd) and reverse-mode Jacobian (jacrev)\n",
    "J_fwd = jacfwd(F)(x)   # shape (2, 3)\n",
    "J_rev = jacrev(F)(x)   # shape (2, 3)\n",
    "\n",
    "print(\"J_fwd (forward-mode):\")\n",
    "print(J_fwd)\n",
    "print(\"\\nJ_rev (reverse-mode):\")\n",
    "print(J_rev)\n",
    "\n",
    "print(\"\\nMax abs difference:\", (J_fwd - J_rev).abs().max().item())\n",
    "print(\"Allclose? ->\", torch.allclose(J_fwd, J_rev, atol=1e-6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0a13b",
   "metadata": {},
   "source": [
    "## Manual Forward-mode vs Reverse-mode Jacobian Construction\n",
    "\n",
    "This example builds the Jacobian of the map $F: \\mathbb{R}^3 \\to \\mathbb{R}^2$ using the underlying\n",
    "mechanics of forward-mode and reverse-mode automatic differentiation.\n",
    "\n",
    "Forward-mode constructs the Jacobian **column by column**. Each standard basis\n",
    "vector in the input space is pushed through `F` via a `jvp`, producing the\n",
    "directional derivative $J e_j$, which forms column $j$ of the Jacobian.\n",
    "\n",
    "Reverse-mode constructs the Jacobian **row by row**. Each standard basis vector\n",
    "in the output space is used as `grad_outputs` in `autograd.grad`, yielding\n",
    "$e_i^\\top J$, which forms row $i$ of the Jacobian.\n",
    "\n",
    "Both Jacobians are assembled explicitly and compared, showing that forward-mode\n",
    "and reverse-mode give the same result, differing only by floating-point\n",
    "precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53992b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Input\n",
    "# ---------------------------------------------------------\n",
    "x = torch.tensor([0.3, -1.2, 0.7], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Dimensions\n",
    "n = 3       # number of inputs x\n",
    "m = 2       # number of outputs F(x)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FORWARD-MODE: iterate over n input directions (columns)\n",
    "# ---------------------------------------------------------\n",
    "J_fwd_cols = []\n",
    "basis_inputs = torch.eye(n)           # e1, e2, e3\n",
    "\n",
    "for j in range(n):\n",
    "    e_j = basis_inputs[j]\n",
    "\n",
    "    # jvp returns (F(x), J(x)*e_j)\n",
    "    _, col_j = jvp(F, (x,), (e_j,))\n",
    "    \n",
    "    J_fwd_cols.append(col_j)\n",
    "\n",
    "J_fwd = torch.stack(J_fwd_cols, dim=1)    # shape (m, n)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# REVERSE-MODE: iterate over m output components (rows)\n",
    "# ---------------------------------------------------------\n",
    "J_rev_rows = []\n",
    "y = F(x)\n",
    "\n",
    "basis_outputs = torch.eye(m)          # e1, e2\n",
    "\n",
    "for i in range(m):\n",
    "    e_i = basis_outputs[i]\n",
    "\n",
    "    # autograd.grad returns (e_i^T * J(x))\n",
    "    grad_row_i_ = torch.autograd.grad(\n",
    "        y,\n",
    "        x,\n",
    "        grad_outputs=e_i,\n",
    "        retain_graph=True,  # ????\n",
    "    )\n",
    "    \n",
    "    print(grad_row_i_)\n",
    "    \n",
    "    grad_row_i = grad_row_i_[0]\n",
    "\n",
    "    J_rev_rows.append(grad_row_i)\n",
    "\n",
    "J_rev = torch.stack(J_rev_rows, dim=0)    # shape (m, n)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Compare the Jacobians\n",
    "# ---------------------------------------------------------\n",
    "print(\"J_fwd (forward-mode, columns):\\n\", J_fwd)\n",
    "print(\"\\nJ_rev (reverse-mode, rows):\\n\", J_rev)\n",
    "print(\"\\nMax abs diff:\", (J_fwd - J_rev).abs().max().item())\n",
    "print(\"Allclose? ->\", torch.allclose(J_fwd, J_rev, atol=1e-6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
