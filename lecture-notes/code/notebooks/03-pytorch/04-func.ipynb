{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad6be05",
   "metadata": {},
   "source": [
    "### The `torch.func` module\n",
    "This notebook demonstrates how to use the `torch.func` module to compute derivatives of functions.\n",
    "\n",
    "The `torch.func` module provides a way to compute derivatives of functions using the `torch.func.grad` and `torch.func.jacrev` functions.\n",
    "\n",
    "The `torch.func.grad` function computes the gradient of a function with respect to a single input tensor.\n",
    "\n",
    "The `torch.func.jacrev` function computes the Jacobian of a function with respect to a single input tensor\n",
    "using reverse-mode automatic differentiation.\n",
    "\n",
    "The `torch.func.jacfwd` function computes the Jacobian of a function with respect to a single input tensor\n",
    "using forward-mode automatic differentiation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde2850",
   "metadata": {},
   "source": [
    "### Scalar function example\n",
    "\n",
    "\n",
    "The next code block calculates derivatives for a batch of data using `torch.func`.\n",
    "\n",
    "* **`f(x)`**: The function to differentiate: $x^2 + \\sin(x^2)$.\n",
    "* **`vmap(grad(f))(x)`**: This is the key operation.\n",
    "    * `grad(f)` computes the gradient for a single scalar.\n",
    "    * `vmap` vectorizes this operation, applying it efficiently across the entire batch `x` (1000 points) in parallel, replacing the need for a Python loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x**2 + torch.sin(x**2)\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 2*x + 2*x*torch.cos(x**2)\n",
    "\n",
    "N = 1000\n",
    "x = torch.linspace(0, 5, N, requires_grad=True)\n",
    "\n",
    "# y.backward(gradient=torch.ones_like(y))\n",
    "dy_dx = torch.func.vmap(torch.func.grad(f))(x)\n",
    "\n",
    "print(\"Are derivatives correct?\", torch.allclose(df_dx(x), dy_dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfb876",
   "metadata": {},
   "source": [
    "### Code Description: Jacobian Computation\n",
    "\n",
    "This script calculates the full Jacobian matrix for a function mapping 2 inputs to 3 \n",
    "outputs ($f: \\mathbb{R}^2 \\to \\mathbb{R}^3$).\n",
    "\n",
    "* **`get_vjp(v)`**: Calculates the gradients for a specific direction `v`.\n",
    "It uses `torch.autograd.grad` to process the entire batch of $N$ points simultaneously.\n",
    "* **`basis_vectors`**: An identity matrix representing the three output components ($y_1, y_2, y_3$).\n",
    "* **`vmap(get_vjp)(basis_vectors)`**:\n",
    "    * Instead of writing a Python loop to calculate the gradient for $y_1$, then $y_2$, then $y_3$ sequentially.\n",
    "    * `vmap` parallelizes this operation, computing the backward pass for all 3 output dimensions at once.\n",
    "* **Result**: `Js` is the Jacobian tensor of shape `(N, 3, 2)`, containing the partial derivatives for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "\n",
    "    y1 = x1 * x2\n",
    "    y2 = torch.sin(x1 + x2**2)\n",
    "    y3 = x1**2 - 3 * x2\n",
    "\n",
    "    return torch.stack((y1, y2, y3), dim=1)\n",
    "\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "\n",
    "    dy1_dx1 = x2\n",
    "    dy1_dx2 = x1\n",
    "\n",
    "    dy2_dx1 = torch.cos(x1 + x2**2)\n",
    "    dy2_dx2 = 2 * x2 * torch.cos(x1 + x2**2)\n",
    "\n",
    "    dy3_dx1 = 2 * x1\n",
    "    dy3_dx2 = -3 * torch.ones_like(x2)\n",
    "\n",
    "    J = torch.stack(\n",
    "        (\n",
    "            torch.stack((dy1_dx1, dy1_dx2), dim=1),\n",
    "            torch.stack((dy2_dx1, dy2_dx2), dim=1),\n",
    "            torch.stack((dy3_dx1, dy3_dx2), dim=1),\n",
    "        ),\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = (2 * torch.rand(N, 2) - 1).requires_grad_()\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "# ---- Jacobian using vmap + autograd.grad ----\n",
    "\n",
    "# 1. Define the function for a SINGLE basis vector v (shape: 3,)\n",
    "#    This replaces the body of your for-loop.\n",
    "def get_vjp(v):\n",
    "    # Expand the vector v (3,) to the whole batch (N, 3)\n",
    "    # e.g., turn [1, 0, 0] into [[1, 0, 0], [1, 0, 0], ...]\n",
    "    g = v.unsqueeze(0).expand_as(y)\n",
    "    \n",
    "    # Compute gradients for this projection\n",
    "    return torch.autograd.grad(\n",
    "        outputs=y,\n",
    "        inputs=x,\n",
    "        grad_outputs=g,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "\n",
    "# 2. Basis vectors for the 3 output dimensions\n",
    "basis_vectors = torch.eye(3, device=x.device)\n",
    "\n",
    "# 3. Apply vmap\n",
    "#    Input: (3, 3) -> effectively iterates over rows\n",
    "#    Output: Stack of 3 results. Each result is (N, 2).\n",
    "#    Total Output Shape: (3, N, 2)\n",
    "Js_vmap = torch.vmap(get_vjp)(basis_vectors)\n",
    "\n",
    "# 4. Permute to match expected shape (N, 3, 2)\n",
    "Js = Js_vmap.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "dfdx = df_dx(x)\n",
    "\n",
    "print(\"Are Jacobians correct?\", torch.allclose(dfdx, Js))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82506533",
   "metadata": {},
   "source": [
    "### Code Description: Batch Jacobian\n",
    "\n",
    "This script computes the Jacobian matrix for a function mapping 2 inputs to 3 outputs ($f: \\mathbb{R}^2 \\to \\mathbb{R}^3$).\n",
    "\n",
    "* **`f(x)`**: Defines the logic for a **single** sample vector.\n",
    "* **`jacrev(f)`**: Uses reverse-mode automatic differentiation to compute the Jacobian matrix for one input.\n",
    "* **`vmap(jacrev(f))(x)`**: This composes the transformations:\n",
    "    * It takes the single-sample Jacobian function.\n",
    "    * It vectorizes it over the batch `x` (1000 samples), computing all Jacobians in parallel without a loop.\n",
    "    * **Result**: A tensor of shape `(1000, 3, 2)` containing the partial derivatives for every sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c56d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "\n",
    "    y1 = x1 * x2\n",
    "    y2 = torch.sin(x1 + x2**2)\n",
    "    y3 = x1**2 - 3 * x2\n",
    "\n",
    "    return torch.stack((y1, y2, y3))\n",
    "\n",
    "\n",
    "def df_dx(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1 = x[:, 0]\n",
    "    x2 = x[:, 1]\n",
    "\n",
    "    dy1_dx1 = x2\n",
    "    dy1_dx2 = x1\n",
    "\n",
    "    dy2_dx1 = torch.cos(x1 + x2**2)\n",
    "    dy2_dx2 = 2 * x2 * torch.cos(x1 + x2**2)\n",
    "\n",
    "    dy3_dx1 = 2 * x1\n",
    "    dy3_dx2 = -3 * torch.ones_like(x2)\n",
    "\n",
    "    J = torch.stack(\n",
    "        (\n",
    "            torch.stack((dy1_dx1, dy1_dx2), dim=1),\n",
    "            torch.stack((dy2_dx1, dy2_dx2), dim=1),\n",
    "            torch.stack((dy3_dx1, dy3_dx2), dim=1),\n",
    "        ),\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "x = (2 * torch.rand(N, 2) - 1).requires_grad_()\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "\n",
    "#    Compute Jacobian using vmap + jacrev\n",
    "#    jacrev calculates the Jacobian for one sample.\n",
    "#    vmap applies it to the whole batch efficiently.\n",
    "Js = torch.func.vmap(torch.func.jacrev(f))(x)\n",
    "\n",
    "\n",
    "dfdx = df_dx(x)\n",
    "\n",
    "print(\"Shapes:\", dfdx.shape, Js.shape)\n",
    "print(\"Are Jacobians correct?\", torch.allclose(dfdx, Js, atol=1e-5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
