{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe81178",
   "metadata": {},
   "source": [
    "- Demonstrates differences between PyTorch and Flax for neural network models in JAX\n",
    "- Shows how to define a simple multilayer perceptron (MLP) in both frameworks\n",
    "- Highlights stateful (PyTorch) vs stateless/lazy (Flax) parameter handling\n",
    "- Includes code for model instantiation and discussion of where parameters \"live\"\n",
    "- Useful for users transitioning from PyTorch to JAX + Flax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dde7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn  # Flax\n",
    "from flax.core import freeze\n",
    "import torch                  # PyTorch\n",
    "import torch.nn as tnn\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47e443",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8883dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PYTORCH (Stateful) ---\n",
    "class TorchMLP(tnn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        # Layers are created AND weights are initialized here immediately.\n",
    "        # The weights live inside 'self.fc1' and 'self.fc2'.\n",
    "        self.fc1 = tnn.Linear(10, 128)\n",
    "        self.fc2 = tnn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tnn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# --- FLAX (Stateless) ---\n",
    "class FlaxMLP(nn.Module):\n",
    "    output_dim: int  # Type annotation (dataclass style)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Layers are defined lazily.\n",
    "        # No weights exist here yet. This is just a graph description.\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b33bf5",
   "metadata": {},
   "source": [
    "#### Instantiation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6538bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PYTORCH ---\n",
    "# Instantiation = Initialization.\n",
    "# Random weights are generated immediately upon creation.\n",
    "torch_model = TorchMLP(output_dim=1)\n",
    "print(f\"PyTorch: Model created. Weights are inside.\")\n",
    "\n",
    "# --- FLAX ---\n",
    "# Instantiation = Configuration.\n",
    "# No weights are generated yet.\n",
    "flax_model = FlaxMLP(output_dim=1)\n",
    "print(f\"Flax:    Blueprint created. No weights yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943c3f0",
   "metadata": {},
   "source": [
    "#### Weights generation in Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d7b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flax needs a PRNGKey and Dummy Input\n",
    "key = jax.random.PRNGKey(0)\n",
    "dummy_input = jnp.ones((1, 10))\n",
    "\n",
    "# The 'init' function returns the State (Pytree)\n",
    "flax_params = flax_model.init(key, dummy_input)\n",
    "\n",
    "print(\"Flax:    Weights generated explicitly via 'init'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dbf1d",
   "metadata": {},
   "source": [
    "#### Why dummy_input? (Lazy Initialization)\n",
    "\n",
    "Notice that in our Flax model, we defined `nn.Dense(features=128)` but we never said what \n",
    "the input size was.\n",
    "\n",
    "Flax infers the input shape automatically the first time data passes through. \n",
    "\n",
    "\n",
    "To trigger this inference and allocate the correct memory for weights, \n",
    "we must pass a single piece of \"dummy\" data (just zeros or ones) through the model \n",
    "using `init`.\n",
    "\n",
    "If `dummy_input` has shape (1, 50), Flax creates a 50x128 matrix.\n",
    "\n",
    "If `dummy_input` has shape (1, 10), Flax creates a 10x128 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c08f5",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "\n",
    "The `.apply` method in Flax is used to run a forward pass of your model using \n",
    "explicitly provided parameters and any other state. \n",
    "\n",
    "When you define a Flax module like `FlaxMLP`, you typically implement a `__call__` method, \n",
    "which describes how your data flows through the model (the forward computation). \n",
    "\n",
    "However, to actually evaluate the model, Flax separates the *blueprint* \n",
    "(the module and its `__call__` definition) from the *parameters/state* \n",
    "(which are returned by `.init`). \n",
    "\n",
    "The `.apply` method takes the parameters (e.g., `flax_params`) and the input data, \n",
    "and runs the logic you wrote in `__call__`. So, it's effectively a way to say: \n",
    "“use *these* parameters in the `__call__` forward pass, with *this* input”.\n",
    "\n",
    "**Summary:**\n",
    "- `__call__`: You write this to define the forward pass computation.\n",
    "- `.apply(params, x)`: This *invokes* your `__call__`, using the specified `params` and data `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input data\n",
    "x_numpy = np.random.randn(5, 10).astype(np.float32)\n",
    "x_torch = torch.from_numpy(x_numpy)\n",
    "x_jax = jnp.array(x_numpy)\n",
    "\n",
    "# --- PYTORCH ---\n",
    "# Syntax: model(x)\n",
    "# Implicitly: It finds 'self.fc1.weight' internally to do the math.\n",
    "y_torch = torch_model(x_torch)\n",
    "\n",
    "# --- FLAX ---\n",
    "# Syntax: model.apply(params, x)\n",
    "# Explicitly: We must hand it the weights we want to use.\n",
    "y_flax = flax_model.apply(flax_params, x_jax)\n",
    "\n",
    "print(\"Forward pass complete for both.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacad8be",
   "metadata": {},
   "source": [
    "#### Mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PYTORCH (Mutation) ---\n",
    "# We reach into the object and change memory in-place.\n",
    "# Side effect: The 'torch_model' object is permanently changed.\n",
    "#\n",
    "# We use torch.no_grad() here because we're manually modifying the model's parameters \n",
    "# (specifically, setting the bias to zero) outside of a typical training step. \n",
    "# Without torch.no_grad(), PyTorch would try to track this operation in its computation graph, \n",
    "# which is unnecessary and could potentially interfere with gradient calculations later.\n",
    "# Wrapping the assignment in torch.no_grad() temporarily disables autograd, \n",
    "# ensuring that this mutation is not recorded for backpropagation.\n",
    "with torch.no_grad():\n",
    "    torch_model.fc1.bias.fill_(0.0)\n",
    "\n",
    "# --- FLAX (Functional Update) ---\n",
    "# We cannot change 'flax_params' (it is frozen/immutable).\n",
    "# We must create a NEW set of parameters.\n",
    "\n",
    "# 1. Deepcopy the original parameters\n",
    "\n",
    "mutable_params = copy.deepcopy(flax_params)\n",
    "\n",
    "# 2. Modify\n",
    "mutable_params['params']['Dense_0']['bias'] = jnp.zeros((128,))\n",
    "\n",
    "# 3. Refreeze (Pack it back up)\n",
    "new_flax_params = freeze(mutable_params)\n",
    "\n",
    "# Proof: The old params still exist!\n",
    "# We have branched the universe. We have 'flax_params' AND 'new_flax_params'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed747c23",
   "metadata": {},
   "source": [
    "#### Do you know deepcopy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1fac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a nested list (a list containing another list)\n",
    "original = [1, [2, 3], 4]\n",
    "\n",
    "# 2. Make a Shallow Copy and a Deep Copy\n",
    "shallow = copy.copy(original)\n",
    "deep = copy.deepcopy(original)\n",
    "\n",
    "# 3. Modify the nested list in the original\n",
    "original[1][0] = 'CHANGED'\n",
    "\n",
    "# 4. Results\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"Shallow:  {shallow}  <-- Affected (shares the nested list)\")\n",
    "print(f\"Deep:     {deep}     <-- Unaffected (has its own nested list)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b915c1",
   "metadata": {},
   "source": [
    "#### Why this happens\n",
    "\n",
    "**Shallow Copy (copy.copy):** Creates a new list, but references the same inner objects. \n",
    "If you change a mutable object inside (like the inner list), the change shows up in both.\n",
    "\n",
    "**Deep Copy (copy.deepcopy):** Recursively creates copies of the list and everything inside it. \n",
    "The new object is completely independent of the original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
