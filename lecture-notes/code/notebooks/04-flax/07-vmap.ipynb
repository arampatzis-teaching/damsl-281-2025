{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49804a4e",
   "metadata": {},
   "source": [
    "- Create and train an ensemble of neural networks efficiently using `jax.vmap` and PyTorch's functional batching tools\n",
    "- Compare vectorized neural network training in JAX/Flax and PyTorch with functional APIs\n",
    "- Use a simple regression task with synthetic data\n",
    "- Build minimal MLP models in both Flax (JAX) and PyTorch\n",
    "- Illustrate how to initialize, structure, and parallelize computation for multiple models\n",
    "- Highlight similarities and differences in workflow between JAX and PyTorch functional approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torch.func as tfunc\n",
    "\n",
    "# Data (Same for both)\n",
    "x_in = np.linspace(-1, 1, 100)[:, None].astype(np.float32)\n",
    "y_true = 2 * x_in + 1 + np.random.randn(100, 1).astype(np.float32) * 0.1\n",
    "\n",
    "x_jax, y_jax = jnp.array(x_in), jnp.array(y_true)\n",
    "x_torch, y_torch = torch.from_numpy(x_in), torch.from_numpy(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4fd00",
   "metadata": {},
   "source": [
    "#### Define the model in Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373492b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model\n",
    "class FlaxMLP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        return nn.Dense(1)(x)\n",
    "\n",
    "# 2. State Creator (Single Model)\n",
    "def create_state(rng):\n",
    "    model = FlaxMLP()\n",
    "    params = model.init(rng, jnp.ones((1, 1)))['params']\n",
    "    tx = optax.adam(0.01)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "# 3. Vectorized Creation (5 Models)\n",
    "keys = jax.random.split(jax.random.PRNGKey(0), 5)\n",
    "ensemble_state = jax.vmap(create_state)(keys)\n",
    "\n",
    "print(f\"Flax Params Shape: {ensemble_state.params['Dense_0']['kernel'].shape}\")\n",
    "# Output: (5, 1, 1) -> (Ensemble, Input, Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc25dcb",
   "metadata": {},
   "source": [
    "#### Define the training step in Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3a7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Single Step Logic\n",
    "def train_step(state, x, y):\n",
    "    def loss_fn(params):\n",
    "        pred = state.apply_fn({'params': params}, x)\n",
    "        return jnp.mean((pred - y) ** 2)\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    return state.apply_gradients(grads=grads), loss\n",
    "\n",
    "# 2. Vectorize it\n",
    "# Map over state (axis 0), broadcast data (None)\n",
    "# `in_axes` specifies how the inputs are mapped across the leading axis during `vmap`.\n",
    "# Here, `state` has shape (ensemble, ...), so we set in_axes=0 for it to map across models,\n",
    "# while `x` and `y` are broadcasted (same batch for all models), so we use None for those.\n",
    "ensemble_step = jax.jit(jax.vmap(train_step, in_axes=(0, None, None)))\n",
    "\n",
    "# 3. Run\n",
    "for i in range(100):\n",
    "    ensemble_state, losses = ensemble_step(ensemble_state, x_jax, y_jax)\n",
    "\n",
    "print(f\"Loss: \", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fbe6fe",
   "metadata": {},
   "source": [
    "#### Define the model in functional Pytorch\n",
    "\n",
    "PyTorch objects (self.layer) are stateful. \n",
    "\n",
    "To vmap them, we must \"extract\" the state and turn the model into a pure function \n",
    "using `torch.func.functional_call`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Model\n",
    "class TorchMLP(tnn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = tnn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "# 2. Instantiate 5 Models (List of objects)\n",
    "models = [TorchMLP() for _ in range(5)]\n",
    "\n",
    "# 3. Stack Parameters\n",
    "# We must manually extract weights from objects and stack them\n",
    "params, buffers = tfunc.stack_module_state(models)\n",
    "\n",
    "# Now 'params' is a dictionary of stacked tensors!\n",
    "# params['layer.weight'].shape -> (5, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b41f84",
   "metadata": {},
   "source": [
    "#### Define the training step in functional Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Pure Function\n",
    "# We need a function: f(params, x) -> loss\n",
    "def compute_loss(params, buffers, x, y):\n",
    "    # functional_call(model, params, input) temporarily \"inserts\" params\n",
    "    pred = tfunc.functional_call(models[0], (params, buffers), (x,))\n",
    "    return torch.mean((pred - y) ** 2)\n",
    "\n",
    "# 2. Vectorize the Function\n",
    "# PyTorch uses 'in_dims', not 'in_axes'\n",
    "# Params (0), Buffers (0), X (None/Broadcast), Y (None/Broadcast)\n",
    "vmap_loss = torch.vmap(compute_loss, in_dims=(0, 0, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple SGD logic\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    # 1. Create a Vectorized Gradient Function\n",
    "    # Why argnums=0? Because 'compute_loss' takes parameters in the order:\n",
    "    # (params, buffers, x, y), so to get gradients w.r.t. 'params' we set argnums=0.\n",
    "    grad_fn = torch.func.grad(compute_loss, argnums=0)\n",
    "    \n",
    "    # Vectorize the gradient computation\n",
    "    # Again: Use in_dims=(0, 0, None, None)\n",
    "    vectorized_grad_fn = torch.vmap(grad_fn, in_dims=(0, 0, None, None))\n",
    "    \n",
    "    # 2. Compute Gradients\n",
    "    grads = vectorized_grad_fn(params, buffers, x_torch, y_torch)\n",
    "    \n",
    "    # 3. Manual Update (SGD)\n",
    "    # Update tensor in-place.\n",
    "    # Note: params is a dictionary of stacked tensors.\n",
    "    with torch.no_grad():\n",
    "        for key in params:\n",
    "            params[key] -= learning_rate * grads[key]\n",
    "\n",
    "print(\"PyTorch Ensemble trained.\")\n",
    "# Verify shapes\n",
    "print(f\"Ensemble Param Shape: {params['layer.weight'].shape}\") \n",
    "# Expected: torch.Size([5, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf912ea",
   "metadata": {},
   "source": [
    "# âš¡ Cheat Sheet: JAX vs. PyTorch Functional API\n",
    "\n",
    "While PyTorch 2.0+ has introduced functional transforms (`torch.func`) that mimic JAX, the API syntax differs slightly.\n",
    "\n",
    "| Concept | **JAX** (`jax`) | **PyTorch** (`torch.func`) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Vectorization** | `jax.vmap(func)` | `torch.vmap(func)` |\n",
    "| **Input Axes Argument** | `in_axes=(0, None)` | `in_dims=(0, None)` |\n",
    "| **Gradient Transform** | `jax.grad(func)` | `torch.func.grad(func)` |\n",
    "| **Value & Gradient** | `jax.value_and_grad(func)` | No direct equivalent (call both or use `grad_and_value` if avail) |\n",
    "| **Extracting Params** | `params = model.init(...)` | `params, buff = torch.func.stack_module_state(models)` |\n",
    "| **Running the Model** | `model.apply(params, x)` | `torch.func.functional_call(model, params, x)` |\n",
    "| **Randomness** | Explicit `key` passed to function | Implicit global state (or `torch.func` specific RNG handling) |\n",
    "\n",
    "**Key Takeaway:**\n",
    "* **JAX** functions are pure by default, so `vmap` and `grad` are native.\n",
    "* **PyTorch** models are objects, so we use `functional_call` and `stack_module_state` to \"force\" them to behave like pure functions for `vmap`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
