{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd996ac",
   "metadata": {},
   "source": [
    "- This notebook demonstrates advanced JAX concepts:\n",
    "  - Automatic differentiation (grad, value_and_grad, higher-order derivatives)\n",
    "  - PyTrees for handling nested data structures\n",
    "  - tree_map for applying functions over PyTrees\n",
    "  - Efficiency with vectorized operations and batching\n",
    "- Includes practical code examples and comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260694a",
   "metadata": {},
   "source": [
    "#### Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 3*x\n",
    "\n",
    "# jax.grad takes a function and returns a NEW function\n",
    "dfdx = jax.grad(f)\n",
    "ddfddx = jax.grad(dfdx) # Second derivative!\n",
    "\n",
    "print(f\"f(2.0):   {f(2.0)}\")      # 10.0\n",
    "print(f\"f'(2.0):  {dfdx(2.0)}\")   # 2*2 + 3 = 7.0\n",
    "print(f\"f''(2.0): {ddfddx(2.0)}\") # 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e05ca",
   "metadata": {},
   "source": [
    "#### Value and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1127003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns tuple: (value, gradient)\n",
    "val, grad = jax.value_and_grad(f)(2.0)\n",
    "print(f\"Loss: {val}, Grad: {grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413fcd2d",
   "metadata": {},
   "source": [
    "#### PyTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a842042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mechanics(x):\n",
    "    return x**2 + 1.0\n",
    "\n",
    "# If we have a batch of particles, standard Python loops are slow.\n",
    "batch = jnp.arange(5.0)\n",
    "\n",
    "# Imagine this is a Neural Network's parameters\n",
    "params = {\n",
    "    'layer1': {'w': jnp.array([1, 2]), 'b': jnp.array([0.1])},\n",
    "    'layer2': {'w': jnp.array([3, 4]), 'b': jnp.array([0.2])}\n",
    "}\n",
    "\n",
    "# We want to double every weight. We can't do params * 2.\n",
    "# We must use tree_map.\n",
    "doubled_params = jax.tree_util.tree_map(lambda x: x * 2, params)\n",
    "\n",
    "print(doubled_params['layer1']['w']) # [2, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6455a3f",
   "metadata": {},
   "source": [
    "# ⚡ Crucial Concept: The PyTree & `tree_map`\n",
    "\n",
    "## 1. What is a PyTree?\n",
    "In JAX and Flax, a **PyTree** is simply a container of data. It is a fancy name for nested structures we use every day in Python:\n",
    "* A `list` is a PyTree.\n",
    "* A `tuple` is a PyTree.\n",
    "* A `dict` is a PyTree.\n",
    "* A `dict` of `lists` of `tuples`... is a PyTree.\n",
    "\n",
    "**Why do we care?**\n",
    "Flax stores your neural network parameters as a nested dictionary (a PyTree).\n",
    "```python\n",
    "params = {\n",
    "    'layer1': {'w': jnp.array([...]), 'b': jnp.array([...])},\n",
    "    'layer2': {'w': jnp.array([...]), 'b': jnp.array([...])}\n",
    "}\n",
    "```\n",
    "\n",
    "## 2. The Problem\n",
    "Standard Python math operators (`+`, `-`, `*`) do not work on dictionaries. You cannot simply multiply your model parameters by 2.\n",
    "\n",
    "```python\n",
    "# ❌ This crashes\n",
    "# params * 2 \n",
    "# TypeError: unsupported operand type(s) for *: 'dict' and 'int'\n",
    "```\n",
    "\n",
    "## 3. The Solution: `jax.tree_util.tree_map`\n",
    "JAX provides a utility that allows you to apply a function to every \"leaf\" (array) in the tree, while preserving the structure.\n",
    "\n",
    "`tree_map(function, tree)` says:\n",
    "> *\"Travel down to every leaf. Apply `function` to that leaf. Rebuild the tree exactly as it was.\"*\n",
    "\n",
    "```python\n",
    "# ✅ The JAX way\n",
    "# Apply \"x * 2\" to every array inside the dictionary\n",
    "doubled_params = jax.tree_util.tree_map(lambda x: x * 2, params)\n",
    "```\n",
    "\n",
    "## 4. The \"Killer App\": Gradient Descent\n",
    "When you train a model, you get a gradient tree (`grads`) that has the exact same structure as your parameter tree (`params`). To update your weights, you need to subtract the gradients from the parameters leaf-by-leaf.\n",
    "\n",
    "`tree_map` can take **multiple trees** as input, as long as they have the same structure.\n",
    "\n",
    "```python\n",
    "# SGD Update Rule: params = params - lr * grads\n",
    "updated_params = jax.tree_util.tree_map(\n",
    "    lambda p, g: p - 0.1 * g,  # The function takes (param, grad)\n",
    "    params,                    # Tree 1 (p)\n",
    "    grads                      # Tree 2 (g)\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
